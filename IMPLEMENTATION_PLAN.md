# üöÄ –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π LLM Router

## üìã –û–±–∑–æ—Ä

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º, –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –≤ –∞—É–¥–∏—Ç–µ. –ü–ª–∞–Ω —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ 3 —ç—Ç–∞–ø–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É.

---

## üî¥ –≠—Ç–∞–ø 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–°–†–û–ß–ù–û)

### –ó–∞–¥–∞—á–∞ 1.1: –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞–∑—Ä—ã–≤ UTF-8 —Å–∏–º–≤–æ–ª–æ–≤
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ –ö–†–ò–¢–ò–ß–ù–û  
**–í—Ä–µ–º—è:** 2-3 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
# –í –Ω–∞—á–∞–ª–µ –∫–ª–∞—Å—Å–∞ ChatService –¥–æ–±–∞–≤–∏—Ç—å:
import codecs

# –í –º–µ—Ç–æ–¥–µ _stream_response_handler (—Å—Ç—Ä–æ–∫–∞ 156):
async def _stream_response_handler(self, response_data: StreamingResponse, provider_type: str, requested_model: str, request_id: str, user_id: str):
    full_content = ""
    stream_completed_usage = None
    
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –°–æ–∑–¥–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä
    utf8_decoder = codecs.getincrementaldecoder('utf-8')(errors='ignore')
    
    try:
        async for chunk in response_data.body_iterator:
            try:
                # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä
                decoded_chunk = utf8_decoder.decode(chunk, final=False)
                
                # –ï—Å–ª–∏ decoded_chunk –ø—É—Å—Ç–æ–π, –∑–Ω–∞—á–∏—Ç chunk –±—ã–ª —á–∞—Å—Ç—å—é –º–Ω–æ–≥–æ–±–∞–π—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
                if not decoded_chunk:
                    continue
                
                # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
```

#### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
```bash
# –¢–µ—Å—Ç —Å emoji –∏ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π
curl -X POST http://localhost:8777/v1/chat/completions \
  -H "Authorization: Bearer dummy" \
  -d '{
    "model": "deepseek/chat",
    "messages": [{"role": "user", "content": "–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Å emoji: —Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python üêç"}],
    "stream": true
  }'
```

---

### –ó–∞–¥–∞—á–∞ 1.2: –î–æ–±–∞–≤–∏—Ç—å –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è SSE —Å–æ–±—ã—Ç–∏–π
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ –ö–†–ò–¢–ò–ß–ù–û  
**–í—Ä–µ–º—è:** 3-4 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
async def _stream_response_handler(self, response_data: StreamingResponse, provider_type: str, requested_model: str, request_id: str, user_id: str):
    full_content = ""
    stream_completed_usage = None
    utf8_decoder = codecs.getincrementaldecoder('utf-8')(errors='ignore')
    
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–µ–ø–æ–ª–Ω—ã—Ö SSE —Å—Ç—Ä–æ–∫
    sse_buffer = ""
    json_buffer = ""
    
    try:
        async for chunk in response_data.body_iterator:
            try:
                decoded_chunk = utf8_decoder.decode(chunk, final=False)
                if not decoded_chunk:
                    continue
                
                # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                if provider_type == "ollama":
                    # –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è NDJSON
                    json_buffer += decoded_chunk
                    lines = json_buffer.split('\n')
                    json_buffer = lines[-1]  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–µ–ø–æ–ª–Ω—É—é —Å—Ç—Ä–æ–∫—É
                    
                    for line in lines[:-1]:
                        if line.strip():
                            processed_chunk, full_content, stream_completed_usage = \
                                self._process_ollama_line(line, full_content, stream_completed_usage, requested_model, request_id, user_id)
                            if processed_chunk:
                                yield processed_chunk
                else:
                    # –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è SSE
                    sse_buffer += decoded_chunk
                    
                    # SSE —Å–æ–±—ã—Ç–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –¥–≤–æ–π–Ω—ã–º \n\n
                    while '\n\n' in sse_buffer:
                        event, sse_buffer = sse_buffer.split('\n\n', 1)
                        if event.strip():
                            full_content, stream_completed_usage = \
                                self._process_openai_sse_event(event, full_content, stream_completed_usage, request_id, user_id)
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º SSE
                            yield f"{event}\n\n".encode('utf-8')
                            
            except json.JSONDecodeError as e:
                logger.error(f"JSONDecodeError in stream for request {request_id}: {e}. Malformed chunk received.", 
                           extra={"request_id": request_id, "user_id": user_id, "log_type": "error", "exception": str(e)})
                yield self._format_sse_error(f"Malformed JSON received from provider: {e}", "malformed_json", status.HTTP_502_BAD_GATEWAY)
                break
            # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ except –±–ª–æ–∫–∏
                
    except Exception as e:
        logger.error(f"Critical error before stream iteration for request {request_id}: {e}", 
                   extra={"request_id": request_id, "user_id": user_id, "log_type": "error", "exception": str(e)}, exc_info=True)
        yield self._format_sse_error(f"A critical error occurred before streaming: {e}", "critical_streaming_error", status.HTTP_500_INTERNAL_SERVER_ERROR)

    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ –≤ –±—É—Ñ–µ—Ä–∞—Ö
    if sse_buffer.strip():
        full_content, stream_completed_usage = self._process_openai_sse_event(sse_buffer, full_content, stream_completed_usage, request_id, user_id)
    if json_buffer.strip():
        try:
            self._process_ollama_line(json_buffer, full_content, stream_completed_usage, requested_model, request_id, user_id)
        except:
            pass
    
    self._log_streaming_completion(request_id, user_id, requested_model, full_content, stream_completed_usage)
    yield b"data: [DONE]\n\n"
```

#### –ù–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã:

```python
def _process_openai_sse_event(self, event: str, full_content: str, stream_completed_usage: Dict[str, Any], request_id: str, user_id: str) -> Tuple[str, Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ SSE —Å–æ–±—ã—Ç–∏–µ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ data: —Å—Ç—Ä–æ–∫)"""
    for line in event.split('\n'):
        if line.startswith('data: '):
            json_data = line[6:].strip()  # –£–±—Ä–∞—Ç—å 'data: '
            if json_data == '[DONE]':
                continue
            try:
                data = json.loads(json_data)
                if 'choices' in data and len(data['choices']) > 0:
                    delta_content = data['choices'][0].get('delta', {}).get('content')
                    if delta_content:
                        full_content += delta_content
                if 'usage' in data:
                    stream_completed_usage = data['usage']
            except json.JSONDecodeError as e:
                raise e
    return full_content, stream_completed_usage

def _process_ollama_line(self, line: str, full_content: str, stream_completed_usage: Dict[str, Any], requested_model: str, request_id: str, user_id: str) -> Tuple[bytes, str, Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É NDJSON –æ—Ç Ollama"""
    processed_chunk = b""
    try:
        data = json.loads(line)
        if data.get('done'):
            if 'prompt_eval_count' in data:
                stream_completed_usage = {
                    "prompt_tokens": data.get("prompt_eval_count", 0),
                    "completion_tokens": data.get("eval_count", 0)
                }
            elif 'usage' in data:
                stream_completed_usage = data['usage']
            return processed_chunk, full_content, stream_completed_usage
        
        delta_content = data.get('message', {}).get('content', '')
        if delta_content:
            full_content += delta_content
        
        openai_chunk = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": requested_model,
            "choices": [{
                "index": 0,
                "delta": {"content": delta_content},
                "logprobs": None,
                "finish_reason": None
            }]
        }
        processed_chunk = f"data: {json.dumps(openai_chunk)}\n\n".encode('utf-8')
    except json.JSONDecodeError as e:
        raise e
    return processed_chunk, full_content, stream_completed_usage
```

---

### –ó–∞–¥–∞—á–∞ 1.3: –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –≤ —Å—Ç—Ä–∏–º–µ
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ –ö–†–ò–¢–ò–ß–ù–û  
**–í—Ä–µ–º—è:** 1-2 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
async def _stream_response_handler(self, response_data: StreamingResponse, provider_type: str, requested_model: str, request_id: str, user_id: str):
    full_content = ""
    stream_completed_usage = None
    utf8_decoder = codecs.getincrementaldecoder('utf-8')(errors='ignore')
    sse_buffer = ""
    json_buffer = ""
    
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –§–ª–∞–≥ –æ—à–∏–±–∫–∏
    stream_has_error = False
    
    try:
        async for chunk in response_data.body_iterator:
            try:
                # ... –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤
            except json.JSONDecodeError as e:
                logger.error(f"JSONDecodeError in stream for request {request_id}: {e}", ...)
                yield self._format_sse_error(f"Malformed JSON: {e}", "malformed_json", status.HTTP_502_BAD_GATEWAY)
                stream_has_error = True  # ‚úÖ –î–û–ë–ê–í–ò–¢–¨
                break
            except ProviderStreamError as e:
                logger.error(f"ProviderStreamError in stream for request {request_id}: {e.message}", ...)
                yield self._format_sse_error(e.message, e.error_code, e.status_code)
                stream_has_error = True  # ‚úÖ –î–û–ë–ê–í–ò–¢–¨
                break
            except ProviderNetworkError as e:
                logger.error(f"ProviderNetworkError in stream for request {request_id}: {e.message}", ...)
                yield self._format_sse_error(e.message, "provider_network_error", status.HTTP_503_SERVICE_UNAVAILABLE)
                stream_has_error = True  # ‚úÖ –î–û–ë–ê–í–ò–¢–¨
                break
            except Exception as e:
                logger.error(f"Unexpected error in stream for request {request_id}: {e}", ...)
                yield self._format_sse_error(f"Unexpected error: {e}", "unexpected_streaming_error", status.HTTP_500_INTERNAL_SERVER_ERROR)
                stream_has_error = True  # ‚úÖ –î–û–ë–ê–í–ò–¢–¨
                break
                
    except Exception as e:
        logger.error(f"Critical error before stream iteration for request {request_id}: {e}", ...)
        yield self._format_sse_error(f"Critical error: {e}", "critical_streaming_error", status.HTTP_500_INTERNAL_SERVER_ERROR)
        stream_has_error = True  # ‚úÖ –î–û–ë–ê–í–ò–¢–¨

    # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –û—Ç–ø—Ä–∞–≤–ª—è—Ç—å [DONE] —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫
    if not stream_has_error:
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ –±—É—Ñ–µ—Ä–æ–≤
        # ...
        self._log_streaming_completion(request_id, user_id, requested_model, full_content, stream_completed_usage)
        yield b"data: [DONE]\n\n"
    else:
        logger.warning(f"Stream terminated with error for request {request_id}, skipping [DONE]", 
                     extra={"request_id": request_id, "user_id": user_id, "log_type": "warning"})
```

---

### –ó–∞–¥–∞—á–∞ 1.4: –£–ª—É—á—à–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –æ—à–∏–±–æ–∫ –¥–ª—è OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ –ö–†–ò–¢–ò–ß–ù–û  
**–í—Ä–µ–º—è:** 1 —á–∞—Å

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
def _format_sse_error(self, message: str, code: str, status_code: int) -> bytes:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –≤ SSE —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å OpenAI API.
    OpenAI –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ SSE —Å–æ–±—ã—Ç–∏–µ, –Ω–µ –∫–∞–∫ chunk.
    """
    # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—à–∏–±–æ–∫ OpenAI
    error_payload = {
        "error": {
            "message": message,
            "type": "api_error",
            "code": code,
            "param": None
        }
    }
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ data: —Å –æ–±—ä–µ–∫—Ç–æ–º error
    return f"data: {json.dumps(error_payload)}\n\n".encode('utf-8')
```

---

## üü° –≠—Ç–∞–ø 2: –£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–í–ê–ñ–ù–û)

### –ó–∞–¥–∞—á–∞ 2.1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç—Ä–∏–º–∞
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü° –í–ê–ñ–ù–û  
**–í—Ä–µ–º—è:** 2-3 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
async def _stream_response_handler(self, response_data: StreamingResponse, provider_type: str, requested_model: str, request_id: str, user_id: str):
    # ... –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
    stream_format = None  # 'sse' –∏–ª–∏ 'ndjson'
    first_chunk = True
    
    try:
        async for chunk in response_data.body_iterator:
            try:
                decoded_chunk = utf8_decoder.decode(chunk, final=False)
                if not decoded_chunk:
                    continue
                
                # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –ø–æ –ø–µ—Ä–≤–æ–º—É —á–∞–Ω–∫—É
                if first_chunk:
                    first_chunk = False
                    if 'data:' in decoded_chunk or decoded_chunk.startswith(':'):
                        stream_format = 'sse'
                        logger.info(f"Detected SSE format for request {request_id}", 
                                  extra={"request_id": request_id, "provider_type": provider_type})
                    else:
                        stream_format = 'ndjson'
                        logger.info(f"Detected NDJSON format for request {request_id}", 
                                  extra={"request_id": request_id, "provider_type": provider_type})
                
                # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤–º–µ—Å—Ç–æ provider_type
                if stream_format == 'ndjson':
                    # NDJSON –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    json_buffer += decoded_chunk
                    # ...
                elif stream_format == 'sse':
                    # SSE –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    sse_buffer += decoded_chunk
                    # ...
                else:
                    # Fallback: –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–∞–Ω–∫ –∫–∞–∫ –µ—Å—Ç—å
                    yield chunk
```

---

### –ó–∞–¥–∞—á–∞ 2.2: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è SSE —Ñ–æ—Ä–º–∞—Ç–∞
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü° –í–ê–ñ–ù–û  
**–í—Ä–µ–º—è:** 2 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è:

```python
def _process_openai_sse_event(self, event: str, full_content: str, stream_completed_usage: Dict[str, Any], request_id: str, user_id: str) -> Tuple[str, Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç SSE —Å–æ–±—ã—Ç–∏–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö SSE –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
    
    event_type = None
    event_data_lines = []
    
    for line in event.split('\n'):
        line = line.strip()
        
        # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∫–∞ SSE –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        if line.startswith(':'):
            continue  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        
        # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∫–∞ event: –ø–æ–ª—è
        if line.startswith('event: '):
            event_type = line[7:].strip()
            continue
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ data: –ø–æ–ª—è
        if line.startswith('data: '):
            data_content = line[6:].strip()
            event_data_lines.append(data_content)
    
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —Ç–∏–ø—É —Å–æ–±—ã—Ç–∏—è
    if event_type == 'error':
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        logger.warning(f"Received error event in SSE stream for request {request_id}", 
                     extra={"request_id": request_id, "event_data": event_data_lines})
        return full_content, stream_completed_usage
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ data
    for json_data in event_data_lines:
        if json_data == '[DONE]':
            continue
        try:
            data = json.loads(json_data)
            
            # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤ data
            if 'error' in data:
                logger.error(f"Error in SSE data for request {request_id}: {data['error']}", 
                           extra={"request_id": request_id, "error_data": data['error']})
                continue
            
            if 'choices' in data and len(data['choices']) > 0:
                delta_content = data['choices'][0].get('delta', {}).get('content')
                if delta_content:
                    full_content += delta_content
            if 'usage' in data:
                stream_completed_usage = data['usage']
        except json.JSONDecodeError as e:
            raise e
    
    return full_content, stream_completed_usage
```

---

### –ó–∞–¥–∞—á–∞ 2.3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤
**–§–∞–π–ª—ã:** `src/providers/base.py`, `src/providers/openai.py`, `src/providers/ollama.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü° –í–ê–ñ–ù–û  
**–í—Ä–µ–º—è:** 1-2 —á–∞—Å–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ `src/providers/base.py`:

```python
async def _stream_request(self, client: httpx.AsyncClient, url_path: str, request_body: Dict[str, Any]) -> StreamingResponse:
    async def generate():
        # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
        stream_timeout = httpx.Timeout(
            connect=10.0,   # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: 10 —Å–µ–∫
            read=30.0,      # –ß—Ç–µ–Ω–∏–µ —á–∞–Ω–∫–∞: 30 —Å–µ–∫ (–º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏)
            write=10.0,     # –ó–∞–ø–∏—Å—å: 10 —Å–µ–∫
            pool=10.0       # Pool: 10 —Å–µ–∫
        )
        
        async with client.stream("POST", f"{self.base_url}{url_path}", 
                                 headers=self.headers, 
                                 json=request_body,
                                 timeout=stream_timeout) as response:  # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨
            # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥
```

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ `src/providers/openai.py`:

```python
async def chat_completions(self, request_body: Dict[str, Any], provider_model_name: str, model_config: Dict[str, Any]) -> Any:
    request_body["model"] = provider_model_name
    options = model_config.get("options")
    if options:
        request_body = deep_merge(request_body, options)

    stream = request_body.get("stream", False)

    try:
        if stream:
            return await self._stream_request(self.client, "/chat/completions", request_body)
        else:
            # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨: –ú–µ–Ω—å—à–∏–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è non-streaming
            non_stream_timeout = httpx.Timeout(
                connect=10.0,
                read=60.0,    # 1 –º–∏–Ω—É—Ç–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                write=10.0,
                pool=10.0
            )
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions", 
                headers=self.headers, 
                json=request_body,
                timeout=non_stream_timeout  # ‚úÖ –ò–ó–ú–ï–ù–ò–¢–¨
            )
            response.raise_for_status()
            return response.json()
```

---

## üü¢ –≠—Ç–∞–ø 3: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)

### –ó–∞–¥–∞—á–∞ 3.1: Backpressure –º–µ—Ö–∞–Ω–∏–∑–º
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü¢ –ñ–ï–õ–ê–¢–ï–õ–¨–ù–û  
**–í—Ä–µ–º—è:** 4-6 —á–∞—Å–æ–≤

#### –ö–æ–Ω—Ü–µ–ø—Ü–∏—è:

```python
import asyncio

async def _stream_response_handler(self, response_data: StreamingResponse, provider_type: str, requested_model: str, request_id: str, user_id: str):
    # ‚úÖ –î–û–ë–ê–í–ò–¢–¨: –û—á–µ—Ä–µ–¥—å –¥–ª—è backpressure
    chunk_queue = asyncio.Queue(maxsize=10)  # –ú–∞–∫—Å 10 —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä–µ
    
    async def producer():
        """–ß–∏—Ç–∞–µ—Ç —á–∞–Ω–∫–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        async for chunk in response_data.body_iterator:
            await chunk_queue.put(chunk)
        await chunk_queue.put(None)  # –°–∏–≥–Ω–∞–ª –æ–∫–æ–Ω—á–∞–Ω–∏—è
    
    async def consumer():
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –∫–ª–∏–µ–Ω—Ç—É"""
        while True:
            chunk = await chunk_queue.get()
            if chunk is None:
                break
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞
            yield processed_chunk
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å producer –≤ —Ñ–æ–Ω–µ
    producer_task = asyncio.create_task(producer())
    
    try:
        async for chunk in consumer():
            yield chunk
    finally:
        producer_task.cancel()
```

---

### –ó–∞–¥–∞—á–∞ 3.2: –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
**–§–∞–π–ª:** `src/services/chat_service.py`  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü¢ –ñ–ï–õ–ê–¢–ï–õ–¨–ù–û  
**–í—Ä–µ–º—è:** 3-4 —á–∞—Å–∞

#### –ö–æ–Ω—Ü–µ–ø—Ü–∏—è:

```python
from dataclasses import dataclass, field
from typing import List

@dataclass
class StreamMetrics:
    request_id: str
    chunks_processed: int = 0
    chunks_failed: int = 0
    bytes_received: int = 0
    unicode_errors: int = 0
    json_errors: int = 0
    start_time: float = field(default_factory=time.time)
    end_time: float = None
    
    def finalize(self):
        self.end_time = time.time()
    
    def duration(self) -> float:
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time
    
    def to_dict(self) -> dict:
        return {
            "request_id": self.request_id,
            "chunks_processed": self.chunks_processed,
            "chunks_failed": self.chunks_failed,
            "bytes_received": self.bytes_received,
            "unicode_errors": self.unicode_errors,
            "json_errors": self.json_errors,
            "duration": self.duration()
        }

async def _stream_response_handler(...):
    metrics = StreamMetrics(request_id=request_id)
    
    try:
        async for chunk in response_data.body_iterator:
            metrics.bytes_received += len(chunk)
            try:
                decoded_chunk = utf8_decoder.decode(chunk, final=False)
                if not decoded_chunk:
                    metrics.unicode_errors += 1
                    continue
                metrics.chunks_processed += 1
                # ...
            except json.JSONDecodeError:
                metrics.json_errors += 1
                metrics.chunks_failed += 1
                # ...
    finally:
        metrics.finalize()
        logger.info(f"Stream metrics for {request_id}", extra=metrics.to_dict())
```

---

## üß™ –ü–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –¢–µ—Å—Ç 1: UTF-8 —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
```bash
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö —á–∞–Ω–∫-—Ä–∞–∑–º–µ—Ä–æ–≤
python test_unicode_streaming.py
```

```python
# test_unicode_streaming.py
import httpx
import asyncio

async def test_unicode_chunking():
    texts = [
        "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! üåç",
        "–≠—Ç–æ —Ç–µ—Å—Ç —Å —ç–º–æ–¥–∑–∏ üöÄüéâüî•",
        "Êó•Êú¨Ë™û„ÉÜ„Çπ„Éà",
        "ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ"
    ]
    
    for text in texts:
        response = await httpx.post(
            "http://localhost:8777/v1/chat/completions",
            headers={"Authorization": "Bearer dummy"},
            json={
                "model": "deepseek/chat",
                "messages": [{"role": "user", "content": f"–ü–æ–≤—Ç–æ—Ä–∏: {text}"}],
                "stream": True
            }
        )
        
        chunks = []
        async for chunk in response.aiter_bytes():
            chunks.append(chunk)
        
        print(f"Test '{text}': {len(chunks)} chunks, success: {response.status_code == 200}")
```

### –¢–µ—Å—Ç 2: –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
```bash
curl -X POST http://localhost:8777/v1/chat/completions \
  -H "Authorization: Bearer dummy" \
  -d '{
    "model": "deepseek/chat",
    "messages": [{"role": "user", "content": "–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ–µ —ç—Å—Å–µ –Ω–∞ 2000 —Å–ª–æ–≤ –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"}],
    "stream": true
  }' | wc -l  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å > 0 –∏ –±–µ–∑ –æ–±—Ä—ã–≤–æ–≤
```

### –¢–µ—Å—Ç 3: –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç
```python
# stress_test.py
import asyncio
import httpx

async def concurrent_requests(n=50):
    async with httpx.AsyncClient() as client:
        tasks = []
        for i in range(n):
            task = client.post(
                "http://localhost:8777/v1/chat/completions",
                headers={"Authorization": "Bearer dummy"},
                json={
                    "model": "deepseek/chat",
                    "messages": [{"role": "user", "content": f"Test {i}"}],
                    "stream": True
                },
                timeout=30.0
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        success = sum(1 for r in responses if not isinstance(r, Exception))
        print(f"Success: {success}/{n}")

asyncio.run(concurrent_requests())
```

---

## üìä –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

### ‚úÖ –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω –µ—Å–ª–∏:
- [ ] –ù–µ—Ç `UnicodeDecodeError` –ø—Ä–∏ –ª—é–±–æ–º —è–∑—ã–∫–µ/emoji
- [ ] –ù–µ—Ç –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑-–∑–∞ —Ä–∞–∑—Ä—ã–≤–∞ SSE/JSON
- [ ] –û—à–∏–±–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –±–µ–∑ `[DONE]`
- [ ] 100% —Ç–µ—Å—Ç–æ–≤ —Å –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –ø—Ä–æ—Ö–æ–¥—è—Ç

### ‚úÖ –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω –µ—Å–ª–∏:
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
- [ ] SSE –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Å–æ–±—ã—Ç–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- [ ] –ù–µ—Ç —Ç–∞–π–º–∞—É—Ç–æ–≤ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ
- [ ] –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç: 95%+ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

### ‚úÖ –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω –µ—Å–ª–∏:
- [ ] Backpressure –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç OOM
- [ ] –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∏–º–æ–≤
- [ ] Dashboard –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

---

## üöÄ –ü–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

1. **–°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É:** `git checkout -b fix/streaming-stability`
2. **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≠—Ç–∞–ø 1** (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
3. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≠—Ç–∞–ø–∞ 1:** –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤
4. **Code Review + Merge –≤ dev**
5. **–î–µ–ø–ª–æ–π –≤ staging** —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
6. **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≠—Ç–∞–ø 2** (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
7. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ + Review + Merge**
8. **–î–µ–ø–ª–æ–π –≤ production** —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º rollout
9. **–≠—Ç–∞–ø 3** - –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö PR

---

## üìù –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ production

- [ ] –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (unit + integration)
- [ ] Code review –ø—Ä–æ–π–¥–µ–Ω (–º–∏–Ω–∏–º—É–º 2 —Ä–µ–≤—å—é–µ—Ä–∞)
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞
- [ ] –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- [ ] –ê–ª–µ—Ä—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –æ—à–∏–±–æ–∫ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
- [ ] Rollback –ø–ª–∞–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω
- [ ] Staging —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
- [ ] Performance —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã
- [ ] Security –∞—É–¥–∏—Ç –ø—Ä–æ–π–¥–µ–Ω (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [AUDIT_REPORT.md](AUDIT_REPORT.md) - –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ–± –∞—É–¥–∏—Ç–µ
- [README.md](README.md) - –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [TODO.md](TODO.md) - –¢–µ–∫—É—â–∏–µ –∑–∞–¥–∞—á–∏

---

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2025-10-03  
**–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏  
**–ö–æ–Ω—Ç–∞–∫—Ç:** @architect-mode